import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.pipeline import make_pipeline

# 1. EL DATASET (Los datos de entrenamiento)
# En la vida real, cargarÃ­as esto de un CSV con pd.read_csv()
data = {
    'mensaje': [
        "Oferta increible gana dinero rapido", # Spam
        "ReuniÃ³n de equipo a las 10am",        # Ham
        "Tu factura ya estÃ¡ disponible",       # Ham
        "Casino online gratis bono bienvenida",# Spam
        "ConfirmaciÃ³n de cita mÃ©dica",         # Ham
        "Pierde peso rÃ¡pido sin dieta"         # Spam
    ],
    'etiqueta': ['spam', 'ham', 'ham', 'spam', 'ham', 'spam']
}
df = pd.DataFrame(data)

print("--- DATOS DE ENTRENAMIENTO ---")
print(df)

# 2. EL MODELO (Pipeline)
# Paso A: CountVectorizer convierte texto en nÃºmeros (Matriz de frecuencias)
# Paso B: MultinomialNB es un algoritmo clÃ¡sico de probabilidad (Bayes)
model = make_pipeline(CountVectorizer(), MultinomialNB())

# 3. ENTRENAMIENTO (AquÃ­ ocurre el aprendizaje)
model.fit(df['mensaje'], df['etiqueta'])
print("\nâœ… Modelo entrenado con Ã©xito.")

# 4. PREDICCIÃ“N (Inferencia)
# Probamos con frases que NUNCA ha visto
nuevos_mensajes = [
    "Hola mamÃ¡, llego tarde a comer",
    "Gana un iphone gratis haciendo click aqui",
    "Reporte mensual de ventas adjunto"
]

print("\n--- RESULTADOS DE LA PREDICCIÃ“N ---")
predicciones = model.predict(nuevos_mensajes)

for msg, pred in zip(nuevos_mensajes, predicciones):
    print(f"ðŸ“ '{msg}' \n   -> Clasificado como: {pred.upper()}")