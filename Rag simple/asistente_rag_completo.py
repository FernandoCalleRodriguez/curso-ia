import os
import lancedb
from dotenv import load_dotenv
from google import genai

# 1. Configuraci√≥n
load_dotenv()
client = genai.Client(api_key=os.getenv("GOOGLE_API_KEY"))

def buscar_contexto(query: str, db_path: str = "./lancedb_data") -> str:
    db = lancedb.connect(db_path)
    try:
        tbl = db.open_table("documentos")
    except:
        return ""

    q_res = client.models.embed_content(
        model="text-embedding-004",
        contents=query
    )
    q_vec = [float(x) for x in q_res.embeddings[0].values]

    # AUMENTAMOS EL L√çMITE A 10 CHUNKS (Para tener m√°s contexto)
    results = tbl.search(q_vec).limit(10).to_pandas()
    
    contexto_unificado = ""
    
    print("\n--- DEBUG: LO QUE LA IA EST√Å LEYENDO ---") 
    for i, row in results.iterrows():
        # Imprimimos los primeros 100 caracteres de cada hallazgo
        print(f"[{i}] {row['text'][:100]}...") 
        contexto_unificado += f"\nFragmento {i}: {row['text']}\n"
        
    print("----------------------------------------\n")
    return contexto_unificado

# --- CAPA DE GENERACI√ìN (LLM) ---
def generar_respuesta(query: str, contexto: str):
    """El cerebro: Combina la pregunta con los datos recuperados."""
    
    if not contexto:
        return "No tengo informaci√≥n en mi base de datos sobre este tema."

    # PROMPT DE ARQUITECTURA (RAG)
    # Le damos personalidad y reglas estrictas (Grounding)
    prompt = f"""
    Eres un Asistente T√©cnico experto en IA.
    Tu misi√≥n es responder a la pregunta del usuario BAS√ÅNDOTE SOLO en el contexto proporcionado.
    
    CONTEXTO RECUPERADO DE LA BASE DE CONOCIMIENTO:
    {contexto}
    
    PREGUNTA DEL USUARIO:
    "{query}"
    
    INSTRUCCIONES:
    1. Si la respuesta est√° en el contexto, expl√≠cala detalladamente.
    2. Si el contexto menciona herramientas o lenguajes espec√≠ficos (Python, Java, etc.), c√≠talos.
    3. Si la respuesta NO est√° en el contexto, di: "El documento no menciona nada espec√≠fico sobre eso".
    4. Ignora pies de p√°gina, cookies o texto irrelevante del contexto.
    """

    print("ü§ñ Generando respuesta con Gemini...")
    response = client.models.generate_content(
        model="gemini-flash-latest",
        contents=prompt
    )
    return response.text

# --- MAIN ---
def main():
    print("--- SISTEMA RAG COMPLETO (LanceDB + Gemini) ---")
    print("üß† Memoria cargada desde ./lancedb_data")
    
    while True:
        query = input("\nPregunta al Experto (o 'salir'): ")
        if query.lower() in ['salir', 'exit']:
            break
            
        # PASO 1: RETRIEVAL (B√∫squeda)
        print("üîç Buscando en la base de datos...")
        contexto = buscar_contexto(query)
        
        if contexto:
            # PASO 2: GENERATION (S√≠ntesis)
            respuesta = generar_respuesta(query, contexto)
            
            print("\n" + "="*50)
            print(f"RESPUESTA GENERADA:")
            print("="*50)
            print(respuesta)
            print("-" * 50)
        else:
            print("‚ùå Error: No se encontr√≥ la base de datos o est√° vac√≠a. Ejecuta el indexador primero.")

if __name__ == "__main__":
    main()